WEBVTT

1
00:00:00.000 --> 00:00:02.870
Okay. This is the last lab that we're going to look at

2
00:00:02.870 --> 00:00:05.210
as part of this module in generalization and sampling,

3
00:00:05.210 --> 00:00:07.110
and it's pretty comprehensive.

4
00:00:07.110 --> 00:00:09.320
So, if it took you quite a while to work through it

5
00:00:09.320 --> 00:00:11.680
and work through all the steps, that's completely expected.

6
00:00:11.680 --> 00:00:13.805
So, let's take a look at a solution walk through right now.

7
00:00:13.805 --> 00:00:15.255
If you haven't attempted it yet,

8
00:00:15.255 --> 00:00:17.445
go ahead and try to pull up the data lab notebook,

9
00:00:17.445 --> 00:00:21.515
IPython notebook yourself, and run through the code that you see there in the cells,

10
00:00:21.515 --> 00:00:23.890
and then come back to this solution walk through video.

11
00:00:23.890 --> 00:00:25.925
Alright. For those of you sticking around,

12
00:00:25.925 --> 00:00:27.740
let's go ahead and take a look at what we got here.

13
00:00:27.740 --> 00:00:33.560
So here I have pulled up the Google Cloud taxicab estimation notebook and ultimately,

14
00:00:33.560 --> 00:00:36.730
what we want to do, is we want to explore,

15
00:00:36.730 --> 00:00:37.980
remember those three steps, right,

16
00:00:37.980 --> 00:00:39.535
we have to explore the data,

17
00:00:39.535 --> 00:00:41.650
we have to create those data sets,

18
00:00:41.650 --> 00:00:44.895
so you're now getting really familiar with how to deal with those hash functions,

19
00:00:44.895 --> 00:00:48.620
and that again those three steps are the training data set,

20
00:00:48.620 --> 00:00:50.480
the evaluation data set and the testing data set,

21
00:00:50.480 --> 00:00:52.890
and the last thing that you might not have seen already,

22
00:00:52.890 --> 00:00:54.895
is how to create a benchmark,

23
00:00:54.895 --> 00:00:58.495
so that we can pummel it later when you learn a lot more about machine learning,

24
00:00:58.495 --> 00:01:00.680
and beat that simplistic model with some of

25
00:01:00.680 --> 00:01:03.030
the more advanced things that you're going to learn in future courses,

26
00:01:03.030 --> 00:01:05.525
like how to build deep neural network using TensorFlow.

27
00:01:05.525 --> 00:01:07.030
So before we do that,

28
00:01:07.030 --> 00:01:09.170
we've got to start from zero and go all the way,

29
00:01:09.170 --> 00:01:10.415
work our way up from the bottom.

30
00:01:10.415 --> 00:01:12.435
So the first thing we need to do as you see here,

31
00:01:12.435 --> 00:01:15.110
is get the data sample.

32
00:01:15.110 --> 00:01:18.805
So, great thing about BigQuery is it got a lot of public data sets.

33
00:01:18.805 --> 00:01:20.625
And much like the flight data,

34
00:01:20.625 --> 00:01:23.420
the taxicab data here is also there.

35
00:01:23.420 --> 00:01:28.785
So what we're going to be doing is pulling all of the cab fares for New York City.

36
00:01:28.785 --> 00:01:30.950
And that's in this public data set,

37
00:01:30.950 --> 00:01:33.320
and the fields that we want to look at, right?

38
00:01:33.320 --> 00:01:35.860
This is a little bit of feature engineering deciding what we're

39
00:01:35.860 --> 00:01:38.650
going to explore and eventually make its way into our model.

40
00:01:38.650 --> 00:01:42.440
Well, if you want to think about the problem of predicting cab fare,

41
00:01:42.440 --> 00:01:44.665
what would be some of the things you'd be interested in?

42
00:01:44.665 --> 00:01:47.880
Well, you want to know potentially when they were picked up,

43
00:01:47.880 --> 00:01:52.255
what's the exact point like the latitude and longitude of the pick up and drop off,

44
00:01:52.255 --> 00:01:54.280
how many people were in the taxicab?

45
00:01:54.280 --> 00:01:56.730
Maybe there of course, there's multiple different fees or

46
00:01:56.730 --> 00:01:59.625
a fare amount tiered structure for the number of occupants,

47
00:01:59.625 --> 00:02:03.750
how long you went, what happens if you cross any of the bridges in New York?

48
00:02:03.750 --> 00:02:05.530
That's the total amount and then you have

49
00:02:05.530 --> 00:02:08.790
the fare amount plus any tips or discretionary spending,

50
00:02:08.790 --> 00:02:10.140
and that's how you get to that total amount.

51
00:02:10.140 --> 00:02:12.840
So we are going to see which of these factors ultimately

52
00:02:12.840 --> 00:02:16.190
play into determining the final fare of a cab ride,

53
00:02:16.190 --> 00:02:17.390
even before you get in,

54
00:02:17.390 --> 00:02:18.795
step foot into that door.

55
00:02:18.795 --> 00:02:21.175
So the first thing we need to do is get the data.

56
00:02:21.175 --> 00:02:23.650
So to get data here in Cloud Datalab,

57
00:02:23.650 --> 00:02:26.130
we're going to invoke a BigQuery query as you see here,

58
00:02:26.130 --> 00:02:28.335
and this is from the BigQuery sample.

59
00:02:28.335 --> 00:02:31.315
So you've got New York City, Yellow Cab trips,

60
00:02:31.315 --> 00:02:35.940
you pulled all those fields that I just mentioned and we're going

61
00:02:35.940 --> 00:02:40.760
to take a look at the very small part of the data.

62
00:02:40.760 --> 00:02:42.880
So we're just going to use much like we used

63
00:02:42.880 --> 00:02:47.305
the one percent sampling in our flights data for the last lab as you saw.

64
00:02:47.305 --> 00:02:50.400
We're going to be using just a small subset of the data here.

65
00:02:50.400 --> 00:02:52.045
So here's the initial query,

66
00:02:52.045 --> 00:02:55.250
and what we want to use is just say,

67
00:02:55.250 --> 00:03:02.395
a 100,000 maybe even we have a 100,000 records to choose from.

68
00:03:02.395 --> 00:03:08.920
Let's see if we can pull out just maybe 10,000 taxicab rides from that.

69
00:03:08.920 --> 00:03:09.975
All right.

70
00:03:09.975 --> 00:03:13.995
So we have kind of parameterized the SQL query.

71
00:03:13.995 --> 00:03:17.420
You can parameterize that much like you would string replacement, right?

72
00:03:17.420 --> 00:03:19.175
So the query is, take

73
00:03:19.175 --> 00:03:23.710
the raw data query because we specified this as raw data up here as you see there,

74
00:03:23.710 --> 00:03:27.835
replace every n, this is grab a record, right?

75
00:03:27.835 --> 00:03:30.385
Sample it every n,

76
00:03:30.385 --> 00:03:34.290
and then the total size we're looking at is a 100,000 records,

77
00:03:34.290 --> 00:03:36.975
and then you're ultimately going to print that query and then execute it.

78
00:03:36.975 --> 00:03:39.195
So this is the query that's executed,

79
00:03:39.195 --> 00:03:45.965
and then we're sampling against this where the remainder of that operation is one,

80
00:03:45.965 --> 00:03:49.305
and so now we're down to only 10,000 taxicab rides.

81
00:03:49.305 --> 00:03:51.850
The reason why we want to do the sampling effort again is,

82
00:03:51.850 --> 00:03:56.000
you don't want to just necessarily take the first 1,000 because that could be ordered,

83
00:03:56.000 --> 00:03:58.050
and you get out bias in your data like say,

84
00:03:58.050 --> 00:04:00.800
a great example for taxicab data is,

85
00:04:00.800 --> 00:04:04.825
it might be sorted by the most recent cab rides have been first.

86
00:04:04.825 --> 00:04:09.550
So if you start looking and exploring your data on the most recent like 3,000 rides,

87
00:04:09.550 --> 00:04:12.330
you can get bias introducing your results because maybe there was

88
00:04:12.330 --> 00:04:16.665
a change or a fare increase that was captured recently,

89
00:04:16.665 --> 00:04:20.105
or a fare decrease that you wouldn't know just by looking at that.

90
00:04:20.105 --> 00:04:22.320
We call it recency bias.

91
00:04:22.320 --> 00:04:24.445
So we've sampled effectively,

92
00:04:24.445 --> 00:04:26.385
and here is what we have.

93
00:04:26.385 --> 00:04:28.340
And this is just, we haven't done anything yet.

94
00:04:28.340 --> 00:04:31.770
This is just the field that we returned from the data sets.

95
00:04:31.770 --> 00:04:34.070
The next step is we want to actually explore it.

96
00:04:34.070 --> 00:04:36.080
So you see we have passenger counts,

97
00:04:36.080 --> 00:04:38.425
you see 1-5 here and some of the examples.

98
00:04:38.425 --> 00:04:41.450
You've got how far you've gotten. Really interesting.

99
00:04:41.450 --> 00:04:45.310
You got zero distance if this is miles for trip distance.

100
00:04:45.310 --> 00:04:46.800
That looks kind of weird.

101
00:04:46.800 --> 00:04:48.995
Zero tolls that can be expected,

102
00:04:48.995 --> 00:04:52.825
fare amount $2.50, and the total amount of $2.50.

103
00:04:52.825 --> 00:04:55.300
OK. So the data looks interesting.

104
00:04:55.300 --> 00:04:57.765
Let's see if we can explore it a little bit quicker.

105
00:04:57.765 --> 00:05:01.470
And the best way to do that, is to say create a data visualization.

106
00:05:01.470 --> 00:05:03.215
So oftentimes in machine learning,

107
00:05:03.215 --> 00:05:07.355
we'll create a scatter plot and take a look at some of the points that we have.

108
00:05:07.355 --> 00:05:11.120
So, here we've plotted trip distance against the fare amount.

109
00:05:11.120 --> 00:05:12.200
So you might be thinking,

110
00:05:12.200 --> 00:05:13.885
well the longer you travel in a cab,

111
00:05:13.885 --> 00:05:15.945
the more that meter rate is going to tick up.

112
00:05:15.945 --> 00:05:19.215
So here we see the longer the trip.

113
00:05:19.215 --> 00:05:23.115
So even a trip distance of 40 here,

114
00:05:23.115 --> 00:05:25.830
you see a general high fare amount of $100.

115
00:05:25.830 --> 00:05:27.890
But you notice two strange,

116
00:05:27.890 --> 00:05:30.655
maybe couple of strange anomalies in the data that you see here.

117
00:05:30.655 --> 00:05:33.705
Well, there's a ton of extremely small trips,

118
00:05:33.705 --> 00:05:35.040
or even trips that could be zero,

119
00:05:35.040 --> 00:05:36.185
because they're right on this line.

120
00:05:36.185 --> 00:05:39.210
That's an anomaly. We want to filter that off of the dataset.

121
00:05:39.210 --> 00:05:41.195
I don't know how you can have a cab ride that doesn't go anywhere.

122
00:05:41.195 --> 00:05:43.195
Maybe you go in and you get immediately kicked out.

123
00:05:43.195 --> 00:05:47.990
And so, you want to look at the points that are zero against this line.

124
00:05:47.990 --> 00:05:51.220
And then maybe any points that have

125
00:05:51.220 --> 00:05:56.285
a look at this kind of solid line that's going up here diagonally here.

126
00:05:56.285 --> 00:05:58.205
It looks almost like a line, but it's actually

127
00:05:58.205 --> 00:06:00.780
a ton of points collected across that line.

128
00:06:00.780 --> 00:06:02.545
That's because of the nature of the data.

129
00:06:02.545 --> 00:06:06.600
It's interesting because in New York when you exit JFK, one of the airports there,

130
00:06:06.600 --> 00:06:10.650
you could actually get a flat rate cab and go pretty much anywhere inside of Manhattan.

131
00:06:10.650 --> 00:06:12.505
And that will actually be a flat rate.

132
00:06:12.505 --> 00:06:14.745
So that's how based on the distance that you're traveling,

133
00:06:14.745 --> 00:06:16.250
it's actually known at that time.

134
00:06:16.250 --> 00:06:17.960
And that's why it's very easy to model

135
00:06:17.960 --> 00:06:20.160
that relationship and that relationship is just a line.

136
00:06:20.160 --> 00:06:23.460
But we want to predict not just folks coming from JFK,

137
00:06:23.460 --> 00:06:26.285
we want to predict folks that are traveling anywhere within New York.

138
00:06:26.285 --> 00:06:29.055
So, interesting things right.

139
00:06:29.055 --> 00:06:32.000
Let's take a look at some of the ways we can preprocess and clean

140
00:06:32.000 --> 00:06:35.840
that data before we ultimately bucket it into the training datasets,

141
00:06:35.840 --> 00:06:37.965
the validation datasets and the testing datasets.

142
00:06:37.965 --> 00:06:40.320
And again you don't want to jump to creating those datasets

143
00:06:40.320 --> 00:06:42.620
splits before you clean your data first.

144
00:06:42.620 --> 00:06:43.830
So garbage in garbage out.

145
00:06:43.830 --> 00:06:45.290
If you start splitting data that's horrible,

146
00:06:45.290 --> 00:06:46.820
you're going to get a horrible model results,

147
00:06:46.820 --> 00:06:50.330
and you're not going to be able to model any behavior that's out there in the real world.

148
00:06:50.330 --> 00:06:52.885
And a good rule of thumb is all data is dirty.

149
00:06:52.885 --> 00:06:54.600
You want to clean and make sure that it's actually in

150
00:06:54.600 --> 00:06:56.500
a good form before you feed it into your model.

151
00:06:56.500 --> 00:06:58.520
Your model only wants good high quality data.

152
00:06:58.520 --> 00:07:02.860
That's what it loves. OK. So, looking at some of the rides here.

153
00:07:02.860 --> 00:07:06.860
So, let's look at anything that has crossed a bridge.

154
00:07:06.860 --> 00:07:09.260
So tolls amount greater than zero.

155
00:07:09.260 --> 00:07:11.900
And then we have a particular day that we're looking at the pickup time.

156
00:07:11.900 --> 00:07:14.780
In this case it's May 20, 2014.

157
00:07:14.780 --> 00:07:17.590
One interesting thing just gazing at the data here,

158
00:07:17.590 --> 00:07:19.280
pick up longitude of zero,

159
00:07:19.280 --> 00:07:21.275
or pick up latitude of zero,

160
00:07:21.275 --> 00:07:25.305
that is clearly wrong, or dirty data.

161
00:07:25.305 --> 00:07:29.210
We need to filter out anything that doesn't actually have a valid pick up location.

162
00:07:29.210 --> 00:07:32.720
So, you want to have a dataset that ultimately makes sense

163
00:07:32.720 --> 00:07:37.075
and doesn't have any records that just look very strange.

164
00:07:37.075 --> 00:07:41.200
Another thing that you might notice here is that the total amount,

165
00:07:41.200 --> 00:07:45.405
nowhere in here do we actually say of the columns that are available to us,

166
00:07:45.405 --> 00:07:48.395
what the customer used as a tip because or

167
00:07:48.395 --> 00:07:51.800
and any cash amount as a tip as well that's not recorded in here.

168
00:07:51.800 --> 00:07:56.525
So, for the purposes of our model since that's unknown and since tips are discretionary,

169
00:07:56.525 --> 00:07:59.445
that's not really included as part of the fare originally.

170
00:07:59.445 --> 00:08:01.025
We're actually not going to predict that.

171
00:08:01.025 --> 00:08:03.930
So, what we're going to do is set the new total amount with

172
00:08:03.930 --> 00:08:08.620
a new fare amount to be just the total amount for the distance that you travel,

173
00:08:08.620 --> 00:08:11.525
plus any tolls that you have.

174
00:08:11.525 --> 00:08:15.780
So, in this particular example the fare amount of 8.5 here is

175
00:08:15.780 --> 00:08:20.055
just the distance that you traveled 2.22 $2 and change,

176
00:08:20.055 --> 00:08:24.490
plus you went over a bridge which is $5.33 that will get you that total fare amount.

177
00:08:24.490 --> 00:08:26.295
So, we going to recalculate that,

178
00:08:26.295 --> 00:08:27.990
just by adding those two together.

179
00:08:27.990 --> 00:08:29.210
And that's going to be a new total amount.

180
00:08:29.210 --> 00:08:32.195
So ignoring tips. All right.

181
00:08:32.195 --> 00:08:36.025
So, an interesting function that you can do is just a .describe,

182
00:08:36.025 --> 00:08:39.620
and that will give you a familiarity for what are some of the bounds,

183
00:08:39.620 --> 00:08:42.420
or some of the ranges of data for the columns that you have,

184
00:08:42.420 --> 00:08:44.070
very useful in statistics.

185
00:08:44.070 --> 00:08:47.670
So, let's take a look at the minimum and maximum for values.

186
00:08:47.670 --> 00:08:49.530
In case it wasn't clear for something like the pickup

187
00:08:49.530 --> 00:08:52.165
longitude or latitude when that was zero,

188
00:08:52.165 --> 00:08:53.960
you can see the max value is zero,

189
00:08:53.960 --> 00:08:55.010
minimum value is zero.

190
00:08:55.010 --> 00:08:57.285
So you can start to look at very strange things.

191
00:08:57.285 --> 00:08:59.765
Some of the things that immediately might become apparent,

192
00:08:59.765 --> 00:09:03.740
is if you have a minimum value for a cab fare that's negative 10.

193
00:09:03.740 --> 00:09:07.240
You can't have a negative cab fare.

194
00:09:07.240 --> 00:09:09.000
No one's paying you money to enter

195
00:09:09.000 --> 00:09:11.245
the cab and take the trip, you got to pay for the ride.

196
00:09:11.245 --> 00:09:13.925
So, and anything that looks like say,

197
00:09:13.925 --> 00:09:16.795
let's find the maximum of passenger count.

198
00:09:16.795 --> 00:09:18.390
Thankfully, this is six right here.

199
00:09:18.390 --> 00:09:21.230
But if you had a max passenger count of say twelve,

200
00:09:21.230 --> 00:09:24.520
there is not a cab vehicle unless it's a bus that was included in this.

201
00:09:24.520 --> 00:09:25.980
That's going to be there as well.

202
00:09:25.980 --> 00:09:28.415
So what we're slowly zeroing in on,

203
00:09:28.415 --> 00:09:30.090
is shaving and cleaning

204
00:09:30.090 --> 00:09:33.610
our whole dataset through an exercise that's called preprocessing.

205
00:09:33.610 --> 00:09:37.300
And then ultimately getting it ready to split into those three buckets,

206
00:09:37.300 --> 00:09:39.670
and then ultimately create a very simple benchmark off of

207
00:09:39.670 --> 00:09:42.220
that that we'll have to beat later on. All right.

208
00:09:42.220 --> 00:09:45.290
So, once you've slogged your way through understanding the data.

209
00:09:45.290 --> 00:09:47.180
And by the way this process could take weeks.

210
00:09:47.180 --> 00:09:48.410
If you're unfamiliar, or you're not

211
00:09:48.410 --> 00:09:51.135
a subject matter expert in the dataset that you're looking at,

212
00:09:51.135 --> 00:09:53.955
and this could be hundreds of columns,

213
00:09:53.955 --> 00:09:55.740
or billions of records,

214
00:09:55.740 --> 00:09:57.440
then engage with an SME,

215
00:09:57.440 --> 00:09:59.475
or subject matter expert that knows the data really well.

216
00:09:59.475 --> 00:10:02.250
And then really understand what the relationships are in the data,

217
00:10:02.250 --> 00:10:03.620
and then visualize it,

218
00:10:03.620 --> 00:10:06.605
use different visualizations, use statistical functions,

219
00:10:06.605 --> 00:10:09.080
even before you get to the machine learning side.

220
00:10:09.080 --> 00:10:11.775
You have to fundamentally understand what's going on in the data.

221
00:10:11.775 --> 00:10:14.105
So, although that took us only five minutes,

222
00:10:14.105 --> 00:10:16.225
the exploration part of machine learning,

223
00:10:16.225 --> 00:10:19.125
understand the datasets could take weeks or even months.

224
00:10:19.125 --> 00:10:23.310
OK. Let's look at some of the individual trips.

225
00:10:23.310 --> 00:10:26.180
So, here we are actually plotting these which is pretty cool,

226
00:10:26.180 --> 00:10:30.480
and you can see the trips themselves where we have the latitude and longitude.

227
00:10:30.480 --> 00:10:32.295
This is the trip lines.

228
00:10:32.295 --> 00:10:37.230
And then you can see that, lines that could be longer typically involve a toll.

229
00:10:37.230 --> 00:10:40.370
And that intuitively makes sense because you're crossing a bridge,

230
00:10:40.370 --> 00:10:42.005
you might be going a longer distance.

231
00:10:42.005 --> 00:10:45.420
It's not like somebody would get in a cab at the beginning of the bridge and

232
00:10:45.420 --> 00:10:49.365
then get out of the cab immediately after the bridge is done.

233
00:10:49.365 --> 00:10:51.260
So that's a good insight.

234
00:10:51.260 --> 00:10:55.020
OK. So, here is actually how we're going to clean up all this data.

235
00:10:55.020 --> 00:10:57.990
So, these are the five insights that we talked a little bit about before.

236
00:10:57.990 --> 00:11:00.770
So, we honed in that New York City longitudes and

237
00:11:00.770 --> 00:11:04.410
latitudes should be within this range negative 74 to 41.

238
00:11:04.410 --> 00:11:06.545
You can't have zero passengers.

239
00:11:06.545 --> 00:11:11.000
So, and arguably you shouldn't have more than a certain set amount,

240
00:11:11.000 --> 00:11:13.820
but we'll just have a baseline of saying no zero passengers.

241
00:11:13.820 --> 00:11:16.310
And then just like what we talked about with the tips,

242
00:11:16.310 --> 00:11:18.660
we're going to recalculate that total amount to

243
00:11:18.660 --> 00:11:22.520
just be fare amount plus the tolls amount as you see here.

244
00:11:22.520 --> 00:11:24.865
And then what we're going to do is,

245
00:11:24.865 --> 00:11:27.450
we know the pick up and drop off locations,

246
00:11:27.450 --> 00:11:29.750
but not the trip distance.

247
00:11:29.750 --> 00:11:33.300
So, this is an interesting pitfall that a lot of

248
00:11:33.300 --> 00:11:37.160
people run into when creating training datasets for machine learning models.

249
00:11:37.160 --> 00:11:38.600
It can't be known.

250
00:11:38.600 --> 00:11:41.445
If it's not known during production time, you can't train on it.

251
00:11:41.445 --> 00:11:44.350
So you can't say something like, all right,

252
00:11:44.350 --> 00:11:48.050
the trip distance was 5 miles, 5.5 miles.

253
00:11:48.050 --> 00:11:51.050
I'm going to say it was a dollar per mile, so therefore,

254
00:11:51.050 --> 00:11:56.035
a very easy simplistic model is that the ultimate trip is going to cost $5.50.

255
00:11:56.035 --> 00:11:57.950
That's because when you start getting new data,

256
00:11:57.950 --> 00:12:00.520
like say I've requested a taxicab.

257
00:12:00.520 --> 00:12:02.690
And then the model asks,

258
00:12:02.690 --> 00:12:04.590
"Okay cool. How long did you travel?"

259
00:12:04.590 --> 00:12:06.660
And you're like, wait a minute. I didn't get in the taxicab.

260
00:12:06.660 --> 00:12:08.750
It's trying to know the future before it happens.

261
00:12:08.750 --> 00:12:09.950
So, you can't date up,

262
00:12:09.950 --> 00:12:11.940
train on data that happens in the future.

263
00:12:11.940 --> 00:12:14.340
So, that's why we're actually dropping this from there,

264
00:12:14.340 --> 00:12:16.465
from the features data set as well.

265
00:12:16.465 --> 00:12:17.910
That's a really important point.

266
00:12:17.910 --> 00:12:20.150
So, think about data that exists,

267
00:12:20.150 --> 00:12:23.445
will exist when you actually launch this inside of production.

268
00:12:23.445 --> 00:12:28.830
So, lots of where clause filters for the BigQuery query that you see here.

269
00:12:28.830 --> 00:12:30.720
We're recalculating the fare amount.

270
00:12:30.720 --> 00:12:32.970
We have the different columns as you can see here.

271
00:12:32.970 --> 00:12:34.680
We're renaming them with aliases,

272
00:12:34.680 --> 00:12:37.435
and we're creating this function, which basically says,

273
00:12:37.435 --> 00:12:38.960
this is going to be

274
00:12:38.960 --> 00:12:41.290
a parameterized query that we're going to

275
00:12:41.290 --> 00:12:44.180
ultimately sample between these particular ranges.

276
00:12:44.180 --> 00:12:47.890
So, here is all of our filters as we talked about a little bit above.

277
00:12:47.890 --> 00:12:52.390
Here's our modulo operators in the Farm Fingerprint hash functions.

278
00:12:52.390 --> 00:12:54.675
We're hashing on pickup_datetime,

279
00:12:54.675 --> 00:12:58.430
and that means the all important message is that whatever you hash on,

280
00:12:58.430 --> 00:13:00.160
be prepared to lose.

281
00:13:00.160 --> 00:13:02.815
So we're willing to part with pickup_date time,

282
00:13:02.815 --> 00:13:05.000
in order for that column to be used in

283
00:13:05.000 --> 00:13:07.815
service for creating the barriers between those buckets.

284
00:13:07.815 --> 00:13:10.520
Training, evaluation, and testing.

285
00:13:10.520 --> 00:13:14.980
And that's ultimately we're saying that the time of day is not

286
00:13:14.980 --> 00:13:21.125
going to have predictive power ultimately in how much the cab fare is going to be.

287
00:13:21.125 --> 00:13:24.930
All right. So, we've created a query that can be parameterized,

288
00:13:24.930 --> 00:13:26.520
and we're going to say,

289
00:13:26.520 --> 00:13:29.240
if that were in training, and ultimately,

290
00:13:29.240 --> 00:13:32.490
what you can think down the road when I loop through this query three times, right?

291
00:13:32.490 --> 00:13:34.180
You got to create three data sets,

292
00:13:34.180 --> 00:13:36.030
training, evaluation and test.

293
00:13:36.030 --> 00:13:37.785
So, if we're in training,

294
00:13:37.785 --> 00:13:39.715
we want 70 percent of the data,

295
00:13:39.715 --> 00:13:42.195
so sample between zero and 70.

296
00:13:42.195 --> 00:13:46.750
And as you can see, sample_between is the query that we created earlier the a, b.

297
00:13:46.750 --> 00:13:50.365
And the a, b get plugged into a and b here,

298
00:13:50.365 --> 00:13:56.640
and that works out on our modulo operator that you see there for every end.

299
00:13:56.640 --> 00:14:03.510
So, for training, that 70 percent validation is between 70 and 85 subtracting those 2,

300
00:14:03.510 --> 00:14:07.350
which means it's an additional 15 percent of the training data set we have available in

301
00:14:07.350 --> 00:14:13.595
the last 15 percent or 85 through 100 is going to be your testing.

302
00:14:13.595 --> 00:14:16.000
Okay. So that gets that all ready to run.

303
00:14:16.000 --> 00:14:19.830
Here's what a query would look like if we ran it,

304
00:14:19.830 --> 00:14:23.215
and now what we're actually going to do,

305
00:14:23.215 --> 00:14:26.270
is specify what the outputs of that are going to be stored.

306
00:14:26.270 --> 00:14:28.310
Because ultimately, we need some kind of say,

307
00:14:28.310 --> 00:14:32.000
CSV files or some other way that the machine learning model can reach out,

308
00:14:32.000 --> 00:14:35.080
and touch and access this training evaluation and testing data.

309
00:14:35.080 --> 00:14:38.480
And to do that, we need to create a function that's going to create these CSVs.

310
00:14:38.480 --> 00:14:41.570
In this particular case, we're training locally.

311
00:14:41.570 --> 00:14:42.820
So within Data Lab,

312
00:14:42.820 --> 00:14:44.895
we're actually going to be storing and creating these CSVs,

313
00:14:44.895 --> 00:14:48.345
in future modules when you get more familiar with Cloud Machine Learning engine,

314
00:14:48.345 --> 00:14:50.910
and using other scalable,

315
00:14:50.910 --> 00:14:54.340
little bit of a prototyping step,

316
00:14:54.340 --> 00:14:57.145
what we're doing here is trying to do all this locally within Cloud Datalab.

317
00:14:57.145 --> 00:15:00.505
But you see they can actually reference data directly from

318
00:15:00.505 --> 00:15:07.695
the query and from Google Cloud Storage directly through like at Google Storage bucket.

319
00:15:07.695 --> 00:15:10.010
Okay. So, here's the CSVs that we're creating.

320
00:15:10.010 --> 00:15:12.140
We're asking them to remove the fare amount,

321
00:15:12.140 --> 00:15:14.975
and then update it with the new one that we have inside of the CSV.

322
00:15:14.975 --> 00:15:16.650
Here's all the features that we're dumping in,

323
00:15:16.650 --> 00:15:21.745
which is pretty much everything that was included in the query up above.

324
00:15:21.745 --> 00:15:23.745
And then here's the all important loop.

325
00:15:23.745 --> 00:15:27.230
For phase in, train, validation,

326
00:15:27.230 --> 00:15:33.015
and testing invoke that query over the sample of 100,000,

327
00:15:33.015 --> 00:15:35.705
and then actually execute that BigQuery query,

328
00:15:35.705 --> 00:15:39.770
and then return the results for data frame that we can then iterate and operate over.

329
00:15:39.770 --> 00:15:42.225
And with those results,

330
00:15:42.225 --> 00:15:48.760
we store that data frame with a prefix taxi dash hyphen,

331
00:15:48.760 --> 00:15:51.060
and then this is going to be your name of your data set.

332
00:15:51.060 --> 00:15:53.890
It's like taxi-train, taxi-validation,

333
00:15:53.890 --> 00:15:58.010
taxi-test inside of storage for the CSVs.

334
00:15:58.010 --> 00:16:00.800
And you can see that's exactly what happens here.

335
00:16:00.800 --> 00:16:03.120
So trust, but verify.

336
00:16:03.120 --> 00:16:05.920
We need make sure that those data sets actually do exist.

337
00:16:05.920 --> 00:16:08.780
So, just doing a simple ls here on the files that we have,

338
00:16:08.780 --> 00:16:15.550
and we see that there is 58,000 cab rides inside of the testing data set.

339
00:16:15.550 --> 00:16:18.890
And we have 400,000 in the training,

340
00:16:18.890 --> 00:16:21.390
and then 100,000 in the validation as well.

341
00:16:21.390 --> 00:16:26.385
So that reflects that split of what we have at the top,

342
00:16:26.385 --> 00:16:29.420
70, 15, and 15.

343
00:16:29.420 --> 00:16:34.260
The interesting part, if you are asking about why the testing

344
00:16:34.260 --> 00:16:39.000
and validation could be different and that's because of the distribution of the data.

345
00:16:39.000 --> 00:16:40.870
And it might not be normally distributed,

346
00:16:40.870 --> 00:16:43.280
so if you have a lot of dates that are clumped together,

347
00:16:43.280 --> 00:16:46.210
and again hashing on one day like January 1,

348
00:16:46.210 --> 00:16:49.410
2018, is going to return the same result as well.

349
00:16:49.410 --> 00:16:50.625
So, the data is not noisy enough,

350
00:16:50.625 --> 00:16:53.465
even if you tell it to be 70, 15, 15,

351
00:16:53.465 --> 00:16:58.700
it's going to hash it in blocks because you might have a lot of

352
00:16:58.700 --> 00:17:01.395
taxicabs that took place on New Year's Day

353
00:17:01.395 --> 00:17:04.380
that it has to fit into one of the different buckets, right?

354
00:17:04.380 --> 00:17:07.615
It can't be bold because you cannot split

355
00:17:07.615 --> 00:17:14.110
one single date what you're hashing on into two different places. Okay, great.

356
00:17:14.110 --> 00:17:18.650
So, let's take a look at the splits.

357
00:17:18.650 --> 00:17:26.305
We do that here, and now that we have all the data ready in those three silo buckets,

358
00:17:26.305 --> 00:17:30.030
it's finally, it's time to actually start creating a,

359
00:17:30.030 --> 00:17:31.570
I'll call this like a dummy model.

360
00:17:31.570 --> 00:17:32.745
This is your benchmark.

361
00:17:32.745 --> 00:17:38.880
So, if you had a simplistic guess of what the cab fare was going to be.

362
00:17:38.880 --> 00:17:41.430
So this is not taking into account weather,

363
00:17:41.430 --> 00:17:44.595
it's not taking into account whether or not you're coming from an airport.

364
00:17:44.595 --> 00:17:46.800
So again, all these more and more complex

365
00:17:46.800 --> 00:17:49.390
features and intuition that you can build into an advanced model,

366
00:17:49.390 --> 00:17:52.210
we're going to save that for later when we learn how to do TensorFlow,

367
00:17:52.210 --> 00:17:54.035
when you learn how to do proper feature engineering.

368
00:17:54.035 --> 00:17:57.680
Right now, is we want to create a pretty simplistic model that says, 'Hey,

369
00:17:57.680 --> 00:18:01.670
your advanced model better beat the RMSE or the loss

370
00:18:01.670 --> 00:18:05.840
metric for the model that we're running here as a benchmark."

371
00:18:05.840 --> 00:18:08.410
So, what does that simple model going to be?

372
00:18:08.410 --> 00:18:10.630
Well we're going to take a look at the,

373
00:18:10.630 --> 00:18:13.310
we're going to need to predict the trip distance first of all.

374
00:18:13.310 --> 00:18:14.910
So, a simple model is going to do that.

375
00:18:14.910 --> 00:18:17.530
And they're going to take the total fare amount,

376
00:18:17.530 --> 00:18:19.370
and divide it over the distance.

377
00:18:19.370 --> 00:18:20.935
And we'll just use a rate,

378
00:18:20.935 --> 00:18:23.350
per mile or per kilometer or something like that.

379
00:18:23.350 --> 00:18:27.150
And then, we want to say, based on the training data set what we know, and again,

380
00:18:27.150 --> 00:18:28.640
in the training data set it is labeled,

381
00:18:28.640 --> 00:18:32.275
meaning we actually do at the end of the day know the fare amount.

382
00:18:32.275 --> 00:18:35.740
That's how we can calculate the loss metric on the data,

383
00:18:35.740 --> 00:18:39.580
and we're going to use RMSE because it's a linear model so floating.

384
00:18:39.580 --> 00:18:42.670
Here's how we actually do that. Okay.

385
00:18:42.670 --> 00:18:45.560
So, we're going to define a couple of different functions to take

386
00:18:45.560 --> 00:18:49.975
the distances between the latitudes and longitudes or the pick up and drop off points.

387
00:18:49.975 --> 00:18:54.055
We're going to then estimate the distance between those two and then get

388
00:18:54.055 --> 00:18:58.965
a figure for how many miles that the taxicab actually drove.

389
00:18:58.965 --> 00:19:02.140
And again, we do know that information in training,

390
00:19:02.140 --> 00:19:03.340
but since we're predicting it,

391
00:19:03.340 --> 00:19:04.760
we cannot use that column.

392
00:19:04.760 --> 00:19:05.780
We're actually going to predict that again.

393
00:19:05.780 --> 00:19:11.000
And then you compute the RMSE as you see with the equation as you see listed there.

394
00:19:11.000 --> 00:19:12.960
And then we are going to print it out,

395
00:19:12.960 --> 00:19:14.890
going to pass in our features to our model.

396
00:19:14.890 --> 00:19:16.950
We actually do want to predict our target.

397
00:19:16.950 --> 00:19:18.795
What we're actually predicting is the fare amount.

398
00:19:18.795 --> 00:19:20.615
We're going to list the features,

399
00:19:20.615 --> 00:19:22.980
and then ultimately, what we're going to do is,

400
00:19:22.980 --> 00:19:26.940
we're going to define where our data frames for training, validation, and test,

401
00:19:26.940 --> 00:19:28.770
those three data sets exist,

402
00:19:28.770 --> 00:19:31.800
and then we're going to train.

403
00:19:31.800 --> 00:19:33.905
Train a very simple model, which says,

404
00:19:33.905 --> 00:19:41.300
predict me a fare amount as the average divided by the,

405
00:19:41.300 --> 00:19:46.050
so the rate that we're calculating is just simply the average of the costs.

406
00:19:46.050 --> 00:19:50.295
So, it was like a 10 dollar cab ride divided by the average of how far it went.

407
00:19:50.295 --> 00:19:52.740
So, the line 28 is

408
00:19:52.740 --> 00:19:57.680
the only place right here where you see any kind of modeling actually happen.

409
00:19:57.680 --> 00:19:59.050
So, we spent 15,

410
00:19:59.050 --> 00:20:00.680
20 minutes going through this lab demo already,

411
00:20:00.680 --> 00:20:04.605
and line 28 is the only place where we actually are doing the prediction or doing the modeling.

412
00:20:04.605 --> 00:20:11.410
So this, it took this long to create the data sets to do the cleaning and preprocessing.

413
00:20:11.410 --> 00:20:15.905
To do the setup of the CSV files for ingestion for the model to make it super easy,

414
00:20:15.905 --> 00:20:19.995
and then ultimately, to have this model be the benchmark for future model performance.

415
00:20:19.995 --> 00:20:24.875
Now, this ratio of 99 percent exploration,

416
00:20:24.875 --> 00:20:27.000
cleaning up, creating in the new data sets,

417
00:20:27.000 --> 00:20:30.235
establishing the benchmarks 99 to one percent of the actual model,

418
00:20:30.235 --> 00:20:33.990
that's going to shift as we start to get into more of model building,

419
00:20:33.990 --> 00:20:35.830
and how to create more sophisticated models,

420
00:20:35.830 --> 00:20:37.565
and how to do feature engineering in the future.

421
00:20:37.565 --> 00:20:40.065
So right now, this just can be our benchmark.

422
00:20:40.065 --> 00:20:44.730
Okay. So, this is the rate per kilometer that we actually get and at the end of the day.

423
00:20:44.730 --> 00:20:52.230
We have a rate of $2.60 per kilometer ride inside of your taxicab.

424
00:20:52.230 --> 00:20:54.665
And here are the RMSEs that you see here,

425
00:20:54.665 --> 00:21:02.585
so we have a training loss metric of 7.45 validation of 9.35,

426
00:21:02.585 --> 00:21:08.530
and actually what we tested it on was surprisingly the best out of all the three at 5.44.

427
00:21:08.530 --> 00:21:14.500
Now, whether or not that is our benchmark, globally saying,

428
00:21:14.500 --> 00:21:17.150
your taxi cab ride is going to be

429
00:21:17.150 --> 00:21:23.515
2.61 per kilometer no matter where you were going doesn't take into account traffic,

430
00:21:23.515 --> 00:21:26.050
doesn't take into account where you're going in Manhattan,

431
00:21:26.050 --> 00:21:27.760
doesn't take into account bridge tolls.

432
00:21:27.760 --> 00:21:29.220
We don't have any parameters in here

433
00:21:29.220 --> 00:21:31.190
for whether or not you're gonna be going over a bridge.

434
00:21:31.190 --> 00:21:33.085
It doesn't take into account time of day.

435
00:21:33.085 --> 00:21:36.240
So, all these things that you were just thinking in the back of your head, hey,

436
00:21:36.240 --> 00:21:38.845
you can't hard code 2.6 times the kilometers,

437
00:21:38.845 --> 00:21:41.920
all that intuition that we're going to build into more sophisticated models,

438
00:21:41.920 --> 00:21:43.280
and at the end of the day,

439
00:21:43.280 --> 00:21:45.905
they better do a much better job hopefully

440
00:21:45.905 --> 00:21:48.985
with all of the additional advanced insights that we're going to build into there,

441
00:21:48.985 --> 00:21:53.880
and we'll revisit this in the future to beat ultimately 5.44.

442
00:21:53.880 --> 00:21:58.690
So, that is your benchmark RMSE to beat. And that's it.

443
00:21:58.690 --> 00:22:00.940
So, the RMSE ultimately,

444
00:22:00.940 --> 00:22:04.725
if we took 5.44 times the actual rate,

445
00:22:04.725 --> 00:22:07.540
this is where you get that nine point.

446
00:22:07.540 --> 00:22:09.070
No, no, sorry.

447
00:22:09.070 --> 00:22:11.520
So, this was actually a little bit different.

448
00:22:11.520 --> 00:22:14.330
This is the 5.44 for this data set here.

449
00:22:14.330 --> 00:22:17.105
And you might get a little bit of the different response.

450
00:22:17.105 --> 00:22:20.175
All right, excellent. So, that wraps up this, this end to end lab,

451
00:22:20.175 --> 00:22:24.495
and I encourage you to continue taking the courses in the specialization.

452
00:22:24.495 --> 00:22:27.675
So ultimately, now that you have, you can't stop here.

453
00:22:27.675 --> 00:22:29.625
Now, that you know how to clean the data,

454
00:22:29.625 --> 00:22:31.020
get the data, ingest it,

455
00:22:31.020 --> 00:22:32.305
build the benchmarking model,

456
00:22:32.305 --> 00:22:34.600
you ultimately need to figure out, well, okay cool.

457
00:22:34.600 --> 00:22:37.220
I'm ready to actually do more sophisticated models and program in

458
00:22:37.220 --> 00:22:40.610
all those cool learning things that the model can

459
00:22:40.610 --> 00:22:42.525
do to make more sophisticated insights

460
00:22:42.525 --> 00:22:44.930
and beat ultimately this model with this RMSE here.

461
00:22:44.930 --> 00:22:48.860
So, stick around for future courses on TensorFlow,

462
00:22:48.860 --> 00:22:52.500
and how to actually beat this RMSE here and feel free.

463
00:22:52.500 --> 00:22:53.960
You have three attempts to do this lab.

464
00:22:53.960 --> 00:22:55.560
So, feel free to repeat it, and edit

465
00:22:55.560 --> 00:22:58.040
the code as you see fit in your Cloud Datalab notebooks.

466
00:22:58.040 --> 00:23:00.000
All right, we'll see around. Nice job.